---
title: "xgbExplainer: HR_data"
author: "Ewelina Karbowiak"
date: "2018-05-07"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.height = 8, fig.width = 8, fig.align = "center")
library("data.table")
library("xgbExplainer")
library("xgboost")
library("Matrix")
library("DALEX")
library("ggradar")
library("ggplot2")
library("ggrepel")
```

Package `xgbExplainer` is the set of tools to explore structure of xgboost and lightGBM models. There are function to finding strong interanctions, to judging importance of single variables and interactions by useage different measure and to compare them. It consists several functions to visualise structure mining of the model.

## Data Info

This vignette shows useage of `xgbExplainer` package to explain xgboost predction model of leaving the company in HR_data. Dataset was taken from [kaggle](https://www.kaggle.com/ludobenistant/hr-analytics) and consists 14999 observations and 10 variables. The dataset is also avaible in package `xgbExplainer` and there it is described more precisely.

```{r, warning=FALSE, message=FALSE}
set.seed(4)
data<-data.table(HR_data, keep.rownames = F)
knitr::kable(head(data))
```

To create correct xgboost model you should remember to change cateriogical features to factors and next change the data frame to sparse matrix. The cateriogical features are one-hot encoded.

```{r, warning=FALSE, message=FALSE}
data<-data[,`:=`(Work_accident=as.factor(Work_accident),promotion_last_5years=as.factor(promotion_last_5years))]
str(data)
sparse_matrix <- Matrix::sparse.model.matrix(left~.-1, data = data)
label = data[,left]==1
head(sparse_matrix)
```


## Xgboost model creation

Package `xgbExplainer` uses table generated by `xgboost::xgb.model.dt.tree` with information about trees, their nodes and leaves.

```{r, warning=FALSE, message=FALSE}
xgb.train.data = xgb.DMatrix(sparse_matrix,label = label,missing = NA)
param <- list(objective = "binary:logistic", base_score = 0.5,max_depth=2)
xgb.model <-xgboost( param = param,data = xgb.train.data,nrounds = 100,verbose = FALSE)
knitr::kable(head(xgboost::xgb.model.dt.tree(colnames(sparse_matrix),xgb.model)))
```

Function `xgboost::xgb.importance` shows importance of single variables. `xgbExplainer` adds new measures of variables importance and shows also importance of interactions.

```{r, warning=FALSE, message=FALSE}
knitr::kable(head(xgboost::xgb.importance(colnames(sparse_matrix),xgb.model)))
```

## Model visualiations


```{r, warning=FALSE, message=FALSE}
lollipopPlot(xgb.model, sparse_matrix)
```


## Interactions finding

We can consider interactions in two ways. In first aproach we can explore all pairs of variable, which occur in the model one above the other . This approach is not the best because we can not distinguish if pair of variables are real interaction or not. In this approach high gain of pair can be a result of high gain of down variable (child). Package has the tools to explore pairs of variables.

```{r, warning=FALSE, message=FALSE}
knitr::kable(head(calculatePairsGainTable(xgb.model,sparse_matrix)))
knitr::kable(head(countPairs(xgb.model,sparse_matrix)))
interactionsPlot(xgb.model,sparse_matrix, opt="pairs")
```

In second approach, to find strong interactions, we can consider only these pairs of variables in which variable on the bottom has higher gain than variable on the top.

```{r, warning=FALSE, message=FALSE}
knitr::kable(head(importanceTable(xgb.model,sparse_matrix,opt="interactions")))
interactionsPlot(xgb.model,sparse_matrix)
```

##Variables' importance

For single variable `xgbExplainer` gives additionally measures of variables importance:

* **counterRoot** - number of occurrences in the root
* **weightedRoot**  - mean number of occurrences in the root, which is weighted by gain
* **meanDepth ** - mean depth weighted by gain


```{r, warning=FALSE, message=FALSE}
knitr::kable(head(importanceTable(xgb.model,sparse_matrix,opt="single")))
```

##Variables' and interactions importance

With `xgbExplainer` package we can compare importance of single variable and interactions.

Funtion `importancePlot` shows two measures of importance, which we can be chosen by `xlab` and `ylab` parameters. It returns three kinds of plots depending on the `opt` parametr:


 * `opt="single"` - plot consists only single variables

 * `opt="interactions"`- only interactions

 * `opt="mixed"`-  plot shows importance both single variables and interactions


 In function `importancePlot` the following measures are avaible:

* **sumGain**
* **sumCover**
* **mean5Gain** - mean gain from 5 occurrences of given variable with the highest gain
* **meanGain**
* **meanCover**
* **freqency**

Additionally for `opt="single"`:

* **meanDepth** (weighted by variables gain)
* **counterRoots**
* **weightedRoot** (weighted by variables gain)

```{r, warning=FALSE, message=FALSE}
knitr::kable(head(importanceTable(xgb.model, sparse_matrix)))
importancePlot(xgb.model, sparse_matrix)
```

The function `importanceRadarPlot` allows us to compare different measures of variables and interteranctions importance on the radar plot from `ggradar` package. Similar to the funtion `importancePlot` it is posible to ganerate this plot in three ways depending on the parametr `opt`.

```{r, warning=FALSE, message=FALSE}
radarPlot(xgb.model,sparse_matrix,top=10, trees=NULL, opt="mixed")
```

##Explanation of one prediction including interactions

```{r, warning=FALSE, message=FALSE}

```
